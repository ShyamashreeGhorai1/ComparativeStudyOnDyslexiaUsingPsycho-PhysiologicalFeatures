# -*- coding: utf-8 -*-
"""RandomForest(PhonologicalAwarnessRelatedFeatures).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xy9ytSX6XUC3T47ULHEhGjUWcFm_i5af

***Importing Data***
"""

import io
import pandas as pd
from google.colab import files

uploaded = files.upload()

df = pd.read_excel(io.BytesIO(uploaded.get('data analysis.xlsx')))

import numpy as np
pd.pandas.set_option('Display.max_columns',None)
pd.pandas.set_option('Display.max_rows',None)

"""**Data Cleaning**"""

df.isnull().sum()

df.duplicated().sum()

# There is no missing and duplicate value in the dataset.

"""***Data Preparation***"""

df = df.drop('Gender',axis=1)

df = df.drop('Age',axis=1)

import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix,roc_auc_score
from sklearn import metrics
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

label_encoder = preprocessing.LabelEncoder()
df['Nativelang']= label_encoder.fit_transform(df['Nativelang'])
df['Otherlang']= label_encoder.fit_transform(df['Otherlang'])
df['Dyslexia']= label_encoder.fit_transform(df['Dyslexia'])

df

"""**Feature Selection**"""

df1 = pd.DataFrame({})

for i in range(1,33):
  if(i<9 or 26<=i<30 or i==31 or i==32):
    df1 = df1.append(df["Clicks"+ str(i)])
    df1 = df1.append(df["Hits"+str(i)])
    df1 = df1.append(df["Misses"+str(i)])
    df1 = df1.append(df["Score"+str(i)])
    df1 = df1.append(df["Accuracy"+str(i)])
    df1 = df1.append(df["Missrate"+str(i)])

df1 = df1.append(df["Dyslexia"])

df2 = df1.T

df2

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(df2.drop(labels=['Dyslexia'],axis=1),df2['Dyslexia'],
                                                                                        test_size=0.2,random_state=0)

"""**Model**"""

from sklearn.ensemble import RandomForestClassifier

# creating a RF classifier
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier()
classifier.fit(x_train, y_train)

#clf = RandomForestClassifier(random_state=42, n_jobs=-1, max_depth=5,
                                      # n_estimators=50, oob_score=True,min_samples_leaf=6)

# Training the model on the training dataset
# fit function is used to train the model using the training sets as parameters
#clf.fit(x_train, y_train)

# performing predictions on the test dataset
#y_pred = clf.predict(x_test)

# metrics are used to find accuracy or error
#from sklearn import metrics
#print()

# using metrics module for accuracy calculation
#print("ACCURACY OF THE MODEL: ", metrics.accuracy_score(y_test, y_pred))

pred = classifier.predict(x_test)
y_train_predict = classifier.predict(x_train)

print("Training Accuracy", accuracy_score(y_train, y_train_predict))
Accuracy = metrics.accuracy_score(y_test, pred)
print("Testing Accuracy", Accuracy)

from sklearn.metrics import classification_report
print(classification_report(y_test, pred))
pd.crosstab(y_test, pred)

"""**Parameter Tuning**"""

classifier = RandomForestClassifier(
    min_samples_split = 50,
    class_weight = "balanced",
    n_estimators = 1000,
    min_samples_leaf = 3,
    n_jobs = -1
)

classifier.fit(x_train, y_train)

pred = classifier.predict(x_test)
y_train_predict = classifier.predict(x_train)

print("Training Accuracy", accuracy_score(y_train, y_train_predict))
Accuracy = metrics.accuracy_score(y_test, pred)
print("Testing Accuracy", Accuracy)

"""**Model Evaluation**"""

from sklearn.metrics import classification_report
print(classification_report(y_test, pred))
pd.crosstab(y_test, pred)

confusion_matrix = metrics.confusion_matrix(y_test, pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()

Precision = metrics.precision_score(y_test, pred)
print(Precision)

Sensitivity_recall = metrics.recall_score(y_test, pred)
print(Sensitivity_recall)

Specificity = metrics.recall_score(y_test, pred, pos_label=0)
print(Specificity)

F1_score = metrics.f1_score(y_test, pred)
print(F1_score)

roc_auc_score(y_test, pred)

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
ns_probs = [0 for _ in range(len(y_test))]
RF_probs = classifier.predict_proba(x_test)
# keep probabilities for the positive outcome only
RF_probs = RF_probs[:, 1]
# calculate scores
ns_auc = roc_auc_score(y_test, ns_probs)
RF_auc = roc_auc_score(y_test, RF_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Random Forest: ROC AUC=%.3f' % (RF_auc))
# calculate roc curves
ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)
RF_fpr, RF_tpr, _ = roc_curve(y_test, RF_probs)
# plot the roc curve for the model
plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
plt.plot(RF_fpr, RF_tpr, marker='.', label='Random Forest')
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
# show the plot
plt.show()